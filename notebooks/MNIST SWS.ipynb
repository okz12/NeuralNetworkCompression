{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:41:17.379583Z",
     "start_time": "2020-06-02T13:41:17.376292Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().split('/')[-1] == \"notebooks\":\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:41:17.767592Z",
     "start_time": "2020-06-02T13:41:17.381592Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import Module\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.model import teacherNet, studentNet\n",
    "from src.sws import GaussianMixturePrior\n",
    "from src.train import train\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from src.kd import extract_logits, kd_ce_loss\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:41:18.029002Z",
     "start_time": "2020-06-02T13:41:17.770958Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data/MNIST/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data/MNIST/', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T13:41:18.041836Z",
     "start_time": "2020-06-02T13:41:18.030762Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet_300_100(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet_300_100, self).__init__()\n",
    "        \n",
    "        self.name = 'LeNet-300-100'\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.train import evaluate\n",
    "import numpy as np \n",
    "from src.sws import sws_prune_copy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_sws_weights_log(model, means=0, precisions=0, epoch=-1, accuracy=-1, savefile = \"\"):\n",
    "\t\"\"\"\n",
    "\tshow model weight histogram with mean and precisions\n",
    "\t\"\"\"\n",
    "\tweights = np.array([], dtype=np.float32)\n",
    "\tfor layer in model.state_dict():\n",
    "\t\tweights = np.hstack( (weights, model.state_dict()[layer].view(-1).cpu().numpy()) )\n",
    "\t\t\n",
    "\tplt.clf()\n",
    "\tplt.figure(figsize=(20, 3))\n",
    "\n",
    "\t#2-Logplot\n",
    "\tsns.distplot(weights, kde=False, color=\"g\",bins=200,norm_hist=True, hist_kws={'log':True})\n",
    "\t#plot mean and precision\n",
    "\tif not (means==0 or precisions==0):\n",
    "\t\tplt.axvline(0, linewidth = 1)\n",
    "\t\tstd_dev0 = np.sqrt(1/np.exp(precisions[0]))\n",
    "\t\tplt.axvspan(xmin=-std_dev0, xmax=std_dev0, alpha=0.3)\n",
    "\n",
    "\t\tfor mean, precision in zip(means, precisions[1:]):\n",
    "\t\t\tplt.axvline(mean, linewidth = 1)\n",
    "\t\t\tstd_dev = np.sqrt(1/np.exp(precision))\n",
    "\t\t\tplt.axvspan(xmin=mean - std_dev, xmax=mean + std_dev, alpha=0.1)\n",
    "\tplt.xlabel(\"Weight Value\")\n",
    "\tplt.ylabel(\"Density\")\n",
    "\tplt.xlim([-1.2, 1.2])\n",
    "\tplt.ylim([1e-3, 1e2])\n",
    "\t\n",
    "\tif savefile!=\"\":\n",
    "\t\tplt.savefig(\"./figs/{}_{}.png\".format(savefile, epoch+1), bbox_inches='tight')\n",
    "\t\tplt.close()\n",
    "\telse:\n",
    "\t\tplt.show()\n",
    "        \n",
    "def get_sparsity(model_prune):\n",
    "\tsp_zeroes = 0\n",
    "\tsp_elem = 0\n",
    "\tfor layer in model_prune.state_dict():\n",
    "\t\tsp_zeroes += float((model_prune.state_dict()[layer].view(-1) == 0).sum())\n",
    "\t\tsp_elem += float(model_prune.state_dict()[layer].view(-1).numel())\n",
    "\tsp = sp_zeroes/sp_elem * 100.0\n",
    "\treturn sp\n",
    "\n",
    "\n",
    "def joint_plot(model, model_orig, gmp, epoch, retraining_epochs, acc, pruned_acc, sparsity, savefile = \"\"):\n",
    "\t\"\"\"\n",
    "\tjoint distribution plot weights before and after sws retraining\n",
    "\t\"\"\"\n",
    "\tweights_T = np.array([], dtype=np.float32)\n",
    "\tfor layer in model.state_dict():\n",
    "\t\tweights_T = np.hstack( (weights_T, model.state_dict()[layer].view(-1).cpu().numpy()) )\n",
    "\n",
    "\tweights_0 = np.array([], dtype=np.float32)\n",
    "\tfor layer in model_orig.state_dict():\n",
    "\t\tweights_0 = np.hstack( (weights_0, model_orig.state_dict()[layer].view(-1).cpu().numpy()) )\n",
    "\n",
    "\t#get mean, stddev\n",
    "\tmu_T = np.concatenate([np.zeros(1), gmp.means.clone().data.cpu().numpy()])\n",
    "\tstd_T = np.sqrt(1/np.exp(gmp.gammas.clone().data.cpu().numpy()))\n",
    "\n",
    "\tx0 = -1.2\n",
    "\tx1 = 1.2\n",
    "\tI = np.random.permutation(len(weights_0))\n",
    "\tf = sns.jointplot(weights_0[I], weights_T[I], size=8, kind=\"scatter\", color=sns.color_palette()[0], stat_func=None, edgecolor='w',\n",
    "\t\t\t\t\t  marker='o', joint_kws={\"s\": 8}, marginal_kws=dict(bins=1000), ratio=4)\n",
    "\tf.ax_joint.hlines(mu_T, x0, x1, lw=0.5)\n",
    "\n",
    "\tfor k in range(len(mu_T)):\n",
    "\t\tif k == 0:\n",
    "\t\t\tf.ax_joint.fill_between(np.linspace(x0, x1, 10), mu_T[k] - 2 * std_T[k], mu_T[k] + 2 * std_T[k],\n",
    "\t\t\t\t\t\t\t\t\tcolor='g', alpha=0.1)\n",
    "\t\telse:\n",
    "\t\t\tf.ax_joint.fill_between(np.linspace(x0, x1, 10), mu_T[k] - 2 * std_T[k], mu_T[k] + 2 * std_T[k],\n",
    "\t\t\t\t\t\t\t\t\tcolor=sns.color_palette()[0], alpha=0.1)\n",
    "\t\n",
    "\tplt.title(\"Epoch: {}/{}\\nTest accuracy: {:.2f}%\\nPrune Accuracy: {:.2f}%\\nSparsity: {:.2f}%\"\n",
    "              .format(epoch+1, retraining_epochs, acc, pruned_acc, sparsity))\n",
    "\tf.ax_marg_y.set_xscale(\"log\")\n",
    "\tf.set_axis_labels(\"Pretrained\", \"Retrained\")\n",
    "\tf.ax_marg_x.set_xlim(-1.2, 1.2)\n",
    "\tf.ax_marg_y.set_ylim(-1.2, 1.2)\n",
    "\tif savefile!=\"\":\n",
    "\t\tplt.savefig(\"./figs/jp_{}_{}.png\".format(savefile, epoch+1), bbox_inches='tight')\n",
    "\t\tplt.close()\n",
    "\telse:\n",
    "\t\tplt.show()\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in testloader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels).sum().item()\n",
    "    model.train()\n",
    "    return 100. * correct / total\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def retrain(model, gmp, dataloader, testloader, optimizer, criterion, epochs=10, writer=None, scheduler=None):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    model_orig = copy.deepcopy(model)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # data = (inputs, targets, teacher_scores(optional))\n",
    "            if torch.cuda.is_available():\n",
    "                data = tuple([x.cuda() for x in data])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data[0].float())\n",
    "            \n",
    "            loss = criterion(outputs, *data[1:])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "                    \n",
    "            \n",
    "        acc = evaluate(model, testloader)\n",
    "        pruned_model = sws_prune_copy(model, gmp, 'l2')\n",
    "        pruned_acc = evaluate(pruned_model, testloader)\n",
    "#         resp_pruned = sws_prune_copy(model, gmp, 'p')\n",
    "#         resp_pruned_acc = evaluate(resp_pruned, testloader)\n",
    "        sparsity = get_sparsity(pruned_model)\n",
    "#         resp_sparsity = get_sparsity(resp_pruned)\n",
    "        \n",
    "        \n",
    "        joint_plot(model, model_orig, gmp, epoch, epochs, acc, pruned_acc, sparsity, savefile = \"lenet\")\n",
    "        show_sws_weights_log(model = model, \n",
    "                             means = list(gmp.means.data.clone().cpu()), \n",
    "                             precisions = list(gmp.gammas.data.clone().cpu()),\n",
    "                            epoch=epoch,\n",
    "                            accuracy=acc,\n",
    "                            savefile=\"lenet\")\n",
    "        print(\"Epoch {} accuracy = {:.2f}% pruned_accuracy = {:.2f}% sparsity = {:.2f}%\".format(epoch + 1, \n",
    "                                                                               acc, \n",
    "                                                                               pruned_acc,\n",
    "                                                                              sparsity,))\n",
    "        pruned_model.cpu()\n",
    "        del pruned_model\n",
    "        if writer:\n",
    "            writer.add_scalar('accuracy', acc, epoch)\n",
    "            writer.add_scalar('training loss', running_loss/len(dataloader), epoch)\n",
    "        \n",
    "        \n",
    "        running_loss = 0.0\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# model = LeNet_300_100().cuda()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-3)\n",
    "\n",
    "# train(model, trainloader, testloader, optimizer, criterion, epochs=30, writer=None)\n",
    "# torch.save(model.state_dict(), \"./models/mnist_lenet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = LeNet_300_100().cuda()\n",
    "orig_model.load_state_dict(torch.load(\"./models/mnist_lenet.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with Soft-weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# logits = extract_logits(orig_model, trainloader).cpu()\n",
    "# kdtrain = torch.utils.data.TensorDataset(trainset.train_data, trainset.train_labels, logits)\n",
    "# kdloader = torch.utils.data.DataLoader(kdtrain, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# opt = torch.optim.Adam(params=model.parameters(), lr=5e-4)\n",
    "\n",
    "# class kd_mse_loss:\n",
    "    \n",
    "#     def __init__(self, temperature, alpha, criterion=nn.CrossEntropyLoss()):\n",
    "#         self.temperature = temperature\n",
    "#         self.alpha = alpha\n",
    "#         self.criterion = criterion\n",
    "#     '''\n",
    "#     Calculate the mse loss between logits_S and logits_T\n",
    "#     :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n",
    "#     :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)\n",
    "#     :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)\n",
    "#     '''\n",
    "    \n",
    "#     def __call__(self, logits_S, labels, logits_T):\n",
    "#         if isinstance(self.temperature, torch.Tensor) and self.temperature.dim() > 0:\n",
    "#             self.temperature = self.temperature.unsqueeze(-1)\n",
    "#         beta_logits_T = logits_T / self.temperature\n",
    "#         beta_logits_S = logits_S / self.temperature\n",
    "#         kd_loss = F.mse_loss(beta_logits_S, beta_logits_T)\n",
    "#         label_loss = self.criterion(logits_S, labels)\n",
    "#         loss = (1.-self.alpha) * label_loss + self.alpha * kd_loss\n",
    "#         print(kd_loss)\n",
    "#         return loss\n",
    "\n",
    "# train(model, kdloader, testloader, opt, kd_ce_loss(temperature=4, alpha=0.8), epochs=30, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-component Mean: 2500.0 Variance: 1250.0\n",
      "Non-zero component Mean: 100.0 Variance: 10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62653b5ae25489f9b076cdc475a1e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Loss: 289535.000\n",
      "Layer Loss: 251.706\n",
      "Layer Loss: 34082.000\n",
      "Layer Loss: 150.885\n",
      "Layer Loss: 811.361\n",
      "Layer Loss: 22.138\n",
      "0-neglogprop Loss: -13828.171\n",
      "Remaining-neglogprop Loss: -39147.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/axisgrid.py:2264: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 accuracy = 98.11% pruned_accuracy = 97.50% sparsity = 26.86%\n",
      "Epoch 2 accuracy = 98.13% pruned_accuracy = 97.60% sparsity = 33.43%\n",
      "Epoch 3 accuracy = 98.17% pruned_accuracy = 97.61% sparsity = 37.32%\n",
      "Epoch 4 accuracy = 98.20% pruned_accuracy = 97.66% sparsity = 40.27%\n",
      "Epoch 5 accuracy = 98.20% pruned_accuracy = 97.76% sparsity = 43.08%\n",
      "Epoch 6 accuracy = 98.25% pruned_accuracy = 97.70% sparsity = 45.61%\n",
      "Epoch 7 accuracy = 98.28% pruned_accuracy = 97.79% sparsity = 47.94%\n",
      "Epoch 8 accuracy = 98.28% pruned_accuracy = 97.78% sparsity = 50.01%\n",
      "Epoch 9 accuracy = 98.29% pruned_accuracy = 97.84% sparsity = 51.97%\n",
      "Epoch 10 accuracy = 98.34% pruned_accuracy = 97.87% sparsity = 53.69%\n",
      "Epoch 11 accuracy = 98.30% pruned_accuracy = 97.93% sparsity = 55.30%\n",
      "Epoch 12 accuracy = 98.31% pruned_accuracy = 98.00% sparsity = 56.83%\n",
      "Epoch 13 accuracy = 98.30% pruned_accuracy = 98.05% sparsity = 58.34%\n",
      "Epoch 14 accuracy = 98.30% pruned_accuracy = 97.99% sparsity = 59.79%\n",
      "Epoch 15 accuracy = 98.26% pruned_accuracy = 97.92% sparsity = 61.26%\n",
      "Epoch 16 accuracy = 98.34% pruned_accuracy = 97.93% sparsity = 62.71%\n",
      "Epoch 17 accuracy = 98.35% pruned_accuracy = 97.99% sparsity = 64.13%\n",
      "Epoch 18 accuracy = 98.34% pruned_accuracy = 98.10% sparsity = 65.41%\n",
      "Epoch 19 accuracy = 98.38% pruned_accuracy = 98.00% sparsity = 66.90%\n",
      "Epoch 20 accuracy = 98.33% pruned_accuracy = 97.93% sparsity = 68.63%\n",
      "Epoch 21 accuracy = 98.31% pruned_accuracy = 97.94% sparsity = 70.90%\n",
      "Epoch 22 accuracy = 98.26% pruned_accuracy = 97.80% sparsity = 72.88%\n",
      "Epoch 23 accuracy = 98.22% pruned_accuracy = 97.65% sparsity = 74.84%\n",
      "Epoch 24 accuracy = 98.05% pruned_accuracy = 97.49% sparsity = 78.47%\n",
      "Epoch 25 accuracy = 97.86% pruned_accuracy = 96.76% sparsity = 84.42%\n",
      "Epoch 26 accuracy = 97.55% pruned_accuracy = 97.24% sparsity = 93.21%\n",
      "Epoch 27 accuracy = 97.67% pruned_accuracy = 97.22% sparsity = 93.86%\n",
      "Epoch 28 accuracy = 97.70% pruned_accuracy = 97.26% sparsity = 93.86%\n",
      "Epoch 29 accuracy = 97.76% pruned_accuracy = 97.20% sparsity = 93.85%\n",
      "Epoch 30 accuracy = 97.78% pruned_accuracy = 97.26% sparsity = 93.85%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = LeNet_300_100().cuda()\n",
    "model.load_state_dict(torch.load(\"./models/mnist_lenet.pt\"))\n",
    "logits = extract_logits(model, trainloader).cpu()\n",
    "kdtrain = torch.utils.data.TensorDataset(trainset.train_data, trainset.train_labels, logits)\n",
    "kdloader = torch.utils.data.DataLoader(kdtrain, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def get_ab(mean, var):\n",
    "\tbeta = mean/var\n",
    "\talpha = mean * beta\n",
    "\treturn (alpha, beta)\n",
    "\n",
    "class SWSLoss:\n",
    "    \n",
    "    def __init__(self, criterion, gmp, tau):\n",
    "        self.criterion = criterion\n",
    "        self.gmp = gmp\n",
    "        self.tau = tau\n",
    "        \n",
    "    def __call__(self, outputs, labels):\n",
    "        acc_loss = self.criterion(outputs, labels)\n",
    "        gmp_loss = self.gmp.call()\n",
    "#         self.tau = self.tau*0.95\n",
    "        return acc_loss + self.tau * gmp_loss\n",
    "\n",
    "n_mixtures = 16\n",
    "zero_mixing_proportion = 0.99\n",
    "gmp = GaussianMixturePrior(nb_components = n_mixtures, \n",
    "                           network_weights = [x for x in model.parameters()], \n",
    "                           pi_zero = zero_mixing_proportion, \n",
    "                           zero_ab = get_ab(2500, 1250),  #(10000, 100)\n",
    "                           ab = get_ab(100, 10), #(1000, 10)\n",
    "                           means = [],\n",
    "                           scaling = False)\n",
    "\n",
    "optimizable_params = [\n",
    "    {'params': model.parameters(), 'lr': 2e-4}, #(5e-3)\n",
    "    {'params': [gmp.means], 'lr': 5e-5}, #(5e-6)\n",
    "    {'params': [gmp.gammas, gmp.rhos], 'lr': 3e-3} #(5e-4)\n",
    "]\n",
    "\n",
    "opt = torch.optim.Adam(optimizable_params)\n",
    "criterion = SWSLoss(nn.CrossEntropyLoss(), gmp, tau=1e-6) #(1e-6)\n",
    "retrain(model, gmp, trainloader, testloader, opt, criterion, epochs=100, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = sws_prune_copy(model, gmp, 'l2')\n",
    "show_sws_weights_log(model = pruned_model, \n",
    "                             means = list(gmp.means.data.clone().cpu()), \n",
    "                             precisions = list(gmp.gammas.data.clone().cpu()),\n",
    "                            epoch=30,\n",
    "                            accuracy=evaluate(pruned_model, testloader),\n",
    "                            savefile=\"lenet\")\n",
    "\n",
    "get_sparsity(pruned_model)\n",
    "\n",
    "import imageio\n",
    "images = []\n",
    "filenames = [\"./figs/lenet_{}.png\".format(x) for x in range(1,101)]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('./figs/lenet_sws_weights.gif', images)\n",
    "\n",
    "images = []\n",
    "filenames = [\"./figs/jp_lenet_{}.png\".format(x) for x in range(1,101)]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('./figs/lenet_jp.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JP](../figs/lenet_jp.gif \"SWS\")\n",
    "\n",
    "![JP](../figs/lenet_sws_weights.gif \"SWS\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
